Intro to R
================

**By [Tim Henderson](https://twitter.com/TimHendersonSL) (tim dot hendo at gmail dot com)**
*Storytelling with Data Workshop at Boston University (June 12, 2017)* <br><br>

\*GitHub repository for Data+Code: <https://github.com/timhendo/R-intro>

# Getting Set Up
Click on the link for the data and code in Github and download the zip file to your hard drive, then decompress to any directory you'll remember - c:/ is fine -- it would appear as c:/R-intro 

* Open RStudio. You should see three panes. 
* Go to File...New File and pick R Script. 

You already have an interactive environment where you can run commands from the script in the console below and see the results. To see this, type an equation at the prompt, then hit the RUN button at the top right. 

...{r}
1+1
...

You can also type 
x=1
and Run, then 
x
To see how it evaluates. 

The point of having a script separate from the console is that you can keep a record of what you did -- make the whole process easy to reproduce for somebody else. That's a big reason you'll want to learn and use R. 

So use File...Open to open the R-intro.R file in the directory we just downloaded. Now you can follow along with the data there.

And be sure to set the working directory so we can see our data. Go to Session...Set Working Directory...To Source File Location. (You could also Choose Directory if you hadn't already loaded a script from that directory)

# Save your data

The panel at top right has two tabs, the first showing the Environment, or all of the “objects” loaded into memory for this R session. We can save this as well, so we don’t have to load and process data again if we return to return to a project later.

Click on the save/disk icon in the Environment panel to save and call the file R-intro.RData.

# Comment your code

Anything that appears on a line after # will be treated as a comment, and will be ignored when the code is run. Get into the habit of commenting your code: Don’t trust yourself to remember what it does!


# Load R packages

Much of the power of R comes from the thousands of “packages” written by its community of open source contributors. These are optimized for specific statistical, graphical or data-processing tasks. To see what packages are available in the basic distribution of R, select the Packages tab in the panel at bottom right. To find packages for particular tasks, try searching Google using appropriate keywords and the phrase “R package.”

In this class, we will work with three incredibly useful packages developed by Hadley Wickham, chief scientist at RStudio:

* readr For reading and writing CSV and other text files.
* dplyr For processing and manipulating data.
* ggplot2 Charting library, which builds graphics in layers according to a standard grammar.

These and several other useful packages have been combined into a super-package called tidyverse.

To install a package, click on the Install icon in the Packages tab, type its name into the dialog box, and make sure that Install dependencies is checked, as some packages will only run correctly if other packages are also installed. Click Install and all of the required packages should install.

Each time you start R, it’s a good idea to click on Update in the Packages panel to update all your installed packages to the latest versions.

Installing a package makes it available to you, but to use it in any R session you need to load it. You can do this by checking its box in the Packages tab. However, we will enter the following code into our script, then highlight these lines of code and run them:

...{r}
# load required packages
library(readr)
library(dplyr)
library(ggplot2)
...

R is a hot topic right now -- data journalists are increasingly learning it as a prime analysis tool, certainly for publishing analyses that can be easily replicated, and this is raising the bar for replicability everywhere.
When the Associated Press recently added new data journalism items to its stylebook, it [specified](www.niemanlab.org/2017/05/the-ap-stylebook-now-includes-new-guidelines-on-data-requesting-it-scraping-it-reporting-on-it-and-publishing-it/) that " “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”

“If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”
One journalism professor I know saw this and reacted this way: "I think this is a knife in the back of old school tinkering in Excel for a while until we get some numbers that look good. How do you replicate noodling around in a spreadsheet for a few hours? You can't. So we're going to have to get better about using methods that can be replicated. "
One of the nice things about R is that it shows every step you take from raw data to finished product -- much the way SAS or in some cases SPSS did, for those familiar with those statistics programs. And it's FREE while SAS and SPSS have become insupportably expensive for most of those outside academia. 


Go to 

Now, this is not written in stone. Many times you can replicate what was done in Excel with careful recordkeeping, and sometimes factcheckers will want to replicate your work even in getting the raw data together -- did you miss anything or fail to get part of the data? It happens! And even without R you would hope somebody could recreate your conclusions knowing nothing but your data source. 
But there's not doubt that there's great peace of mind in code that starts with raw data. I have an ex-colleague, now at the Washington Post, who says "Everytime I decide to just do something real quick in Excel, I always regret it." 
And I know what he means. Often a data request will seem to be a quick table, and you can bypass the steps of importing data, writing queries in SAS or SQL and just paste some numbers into a sheet and work with them a little and hand them over. But so many times you'll get follow-up requests -- "Geen, it would be nice to see this by year going back a ways, or could you just throw in a couple more stats..." 
And as a practical matter this could mean starting over, whereas if you've really datafied it by importing data, it's easy to expand or modify  what you've done, and there's less last-minute worry about mistakes.
I have a friend at Reuters who recommends writing down every sentence in a story based on data, then trying to recreate it again from scratch before publication without any reuse of code. 

So replication is not innoculation against mistakes, but it is a paper trail you can follow to isolate mistakes when they appear and fix thenm without starting over. And that's the real peace of mind. And there's nothing to say you can't use Excel, SQL, Perl or anything else you want to prepare your data for analysis in R. 

Also, with R you can benefit from others' analyses and follow along with the increasoing number of data journalism outlets that publish in R alongside their reports. 
For instance BuzzFeed and FiveThirtyEight keep a lot of their analyses online for anyone to replicate in R. 
* [FiveThirtyEight](https://github.com/fivethirtyeight/data)
* [BuzzFeed](https://github.com/BuzzFeedNews/everything)

# Getting Set up

There are reasons why [R is hard to learn](http://r4stats.com/articles/why-r-is-hard-to-learn/) -- it doesn't follow the mold of other data analysis or statistics programs and has its own syntax and keywords that often don't match what you'd expect, like "sort" -- 

