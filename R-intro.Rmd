Intro to R
================

**By [Tim Henderson](https://twitter.com/TimHendersonSL) (tim dot hendo at gmail dot com)**
*Storytelling with Data Workshop at Boston University (June 12, 2017)* <br><br>

\*GitHub repository for Data+Code: <https://github.com/timhendo/R-intro>

Based on a tip sheet from Peter Aldhous, NICAR 2017, National Institute for Computer Assisted Reporting, with modifications

# Getting Set Up
Click on the link for the data and code in Github and download the zip file to your hard drive, then decompress to any directory you'll remember - c:/ is fine -- it would appear as c:/R-intro 

* Open RStudio. You should see three panes. 
* Go to File...New File and pick R Script. 

You already have an interactive environment where you can run commands from the script in the console below and see the results. To see this, type an equation at the prompt, then hit the RUN button at the top right. 

```{r}
1+1
```

You can also type 
x=1
and Run, then 
x
To see how it evaluates. 

The point of having a script separate from the console is that you can keep a record of what you did -- make the whole process easy to reproduce for somebody else. That's a big reason you'll want to learn and use R. 

So use File...Open to open the R-intro.R file in the directory we just downloaded. Now you can follow along with the data there.

And be sure to set the working directory so we can see our data. Go to Session...Set Working Directory...To Source File Location. (You could also Choose Directory if you hadn't already loaded a script from that directory)

# Save your data
The panel at top right has two tabs, the first showing the Environment, or all of the “objects” loaded into memory for this R session. We can save this as well, so we don’t have to load and process data again if we return to return to a project later.

Click on the save/disk icon in the Environment panel to save and call the file R-intro.RData.

# Comment your code

Anything that appears on a line after # will be treated as a comment, and will be ignored when the code is run. Get into the habit of commenting your code: Don’t trust yourself to remember what it does!


# Load R packages

Much of the power of R comes from the thousands of “packages” written by its community of open source contributors. These are optimized for specific statistical, graphical or data-processing tasks. To see what packages are available in the basic distribution of R, select the Packages tab in the panel at bottom right. To find packages for particular tasks, try searching Google using appropriate keywords and the phrase “R package.”

In this class, we will work with three incredibly useful packages developed by Hadley Wickham, chief scientist at RStudio:

* readr For reading and writing CSV and other text files.
* dplyr For processing and manipulating data.
* ggplot2 Charting library, which builds graphics in layers according to a standard grammar.

These and several other useful packages have been combined into a super-package called tidyverse.

To install a package, click on the Install icon in the Packages tab, type its name into the dialog box, and make sure that Install dependencies is checked, as some packages will only run correctly if other packages are also installed. Click Install and all of the required packages should install.

Each time you start R, it’s a good idea to click on Update in the Packages panel to update all your installed packages to the latest versions.

Installing a package makes it available to you, but to use it in any R session you need to load it. You can do this by checking its box in the Packages tab. However, we will enter the following code into our script, then highlight these lines of code and run them:

```{r}
# load required packages
library(readr)
library(dplyr)
library(ggplot2)
```
# Load data

You can load data into the current R session by selecting Import Dataset>From Text File... in the Environment tab.

However, we will use the read_csv function from the readr package. Copy the following code into your script and Run:

```{r}
# load health and wealth of nations data
nations1 <- read_csv("nations1.csv")
nations2 <- read_csv("nations2.csv")
```


Notice that the Environment now contains two objects, of the type tbl_df, a variety of the standard R object for holding tables of data, known as a data frame:

# Explore the data

The str function will tell you more about the columns in your data, including their data. 
```{r}
# view some of data
head(nations1)

```
The head() function lets you see the first six rows of data and also the data type, which in this scenario is guessed by the read() function. Not bad -- it correctly guess that some columns are character (text) and some are numbers -- though it's made the population column "double" for decimal when it's really an integer (no decimals)

Let's take a look at the population column specifically

```{r}
# print values for population in the nations1 data
nations1$population
```
The output will be the first 10,000 values for that column.

If you need to change the data type for any column, use the following functions:

* as.character converts to a text string.
* as.numeric converts to a number.
* as.factor converts to a categorical variable.
* as.integer converts to an integer
* as.Date converts to a date
* as.POSIXct converts to a full date and timestamp.

So this code will convert the population numbers to integers:

```{r}
# convert population to integers
nations1$population <- as.integer(nations1$population)
```
Now check again and note that population has changed to an integer type
```{r}
head(nations1)
```
Exercise: Try doing the same thing with the year column: look at it specifically, change it to an integer data type, then look at the nations1 data set again to make sure the change was made.

# Summarize the data

The summary function will run a quick statistical summary of a data frame, calculating mean, median and quartile values for continuous variables:
```{r}
# summary of nations1 data
summary(nations1)
```
Uh-oh! What's the income variable that's a character field? Sounds like it should be numeric! Let's check -- what should we do to look at that column specifically? 

# Manipulate and analyze data with dplyr

Now we will use dplyr to manipulate the data, using the following basic operations. If you’ve worked with SQL before, these operations will be very familiar and this is another reason Hadley Wickam is a genius and has turned R into something we all need:

* Sort: Largest to smallest, oldest to newest, alphabetical etc.

* Filter: Select a defined subset of the data.

* Summarize/Aggregate: Deriving one value from a series of other values to produce a summary statistic. Examples include: count, sum, mean, median, maximum, minimum etc. Often you’ll group data into categories first, and then aggregate by group.

* Join: Merging entries from two or more datasets based on common field(s), e.g. unique ID number, last name and first name.
 
For those with SQL backgrounds such as Microsoft Access or MySQL, this introduces some familiar concepts that will make R more hospitable. Here are some of the most useful functions in dplyr:

* select -- Choose which columns to include.
* filter -- Filter the data.
* arrange --Sort the data, by size for continuous variables, by date, or alphabetically.
* group_by-- Group the data by a categorical variable.
* summarize -- Summarize, or aggregate (for each group if following group_by). Often used in conjunction with functions including:
* mean(x) -- Calculate the mean, or average, for variable x.
* median(x) -- Calculate the median.
* max(x) -- Find the maximum value.
* min(x) -- Find the minimum value.
* sum(x) -- Add all the values together.
* n() -- Count the number of records. Here there isn’t a variable in the brackets of the function, because the number of records applies to all variables.
* n_distinct(x)--  Count the number of unique values in variable x.
* mutate -- Create new column(s) in the data, or change existing column(s).
* rename -- Rename column(s).

These functions can be chained together using the “pipe” operator %>%, which makes the output of one line of code the input for the next. This allows you to run through a series of operations in a logical order. I find it helpful to think of %>% as meaning “then.”

# Filter and sort data

Now we will filter and sort the data in specific ways. We create new objects -- the way nations1 is an object or data frame -- to hold our results. First, we'll filster 

First we'll Filter the data for 2014 only and for nations that have life expectancy data only.

```{r}
# filter data for 2014 only
longevity <- nations1 %>%
  filter(year == 2014 & !is.na(life_expect)) %>%
  select(country, life_expect, income, region)
  #check the result
  head(longevity)
  ```
  In this code, we create a new object called longevity from nations1 and then (%>%) filter it for just the data for 2015 and to include only non-null values. Then we select just four variables from the ten in the original data frame. There should be data returned for 195 countries.


Exercise: Wait, didn't we see 2015 as a year? Why are we using older data here? Try doing this for 2015 and see what happens.

Now, let find the top 10 high-income countries with the lowest life expectancy in 2014 

```{r}

# find the ten high-income countries with the lowest life expectancy
high_income_short_life <- longevity %>%
  filter(income == "High income") %>%
  arrange(life_expect) %>%
  head(10)
#Now look at what we have
head(high_income_short_life,10)
```

Next, Find the 20 countries with the longest life expectancy in 2014, plus the United States with its rank, if it lies outside the top 20
```{r)
# find the 20 countries with the longest life expectancies, 
# plus the United States with its rank, if it lies outside the top 20
long_life <- longevity %>%
  arrange(desc(life_expect)) %>%
  mutate(rank = c(1:195)) %>%
  filter(rank <= 20 | country == "United States")
  #take a look
  head(long_life,21)
```
Did that work? It did, but an RStudio element called tibbles prevents us from seeing all 21 rows, so we have to fall back on the print() function 
```[r}
#Tibble won't let us see that many rows so we fall back on print()
print(tbl_df(long_life), n=21)
```

Hopefully the logic and flexibility of dplyr code is now becoming clear. Here we start by sorting the longevity data frame in descending order of life expantancy, then we create a new variable in the data called rank, using the mutate function. By feeding this a list of numbers from 1 to 195, we rank the countries according to their life expectancy. Finally we filter the data for the top 20 countries, plus the United States.

Whereas in the initial filter of the data to create longevity data frame we used & to return data meeting both criteria, this time we used | to include data meeting either criteria. & and | are equivalent to AND and OR in SQL. When combining & and | in more complex filters, use parentheses to determine which parts of the evaluation should be carried out first.

Now let’s find out where Russia ranks, too
```{r}
# find the 20 countries with the longest life expectancies,
# plus the United States and Russia with their ranks
long_life <- longevity %>%
  arrange(desc(life_expect)) %>%
  mutate(rank = c(1:195)) %>%
  filter(rank <= 20 | grepl("United States|Russia", country))
  #Again use print to see all the rows
print(tbl_df(long_life), n=22)
  ```
This extremely geeky examples uses a UNIX style function for simple pattern matching on text, using the function grepl("pattern_a|pattern_b", x), which searches variable x for values containing any of a list of text values. This is useful for fuzzy text matching. Notice how searching for Russia returns Russian Federation, which is the country’s full name. 


# Write data to a CSV file (comma separated values)

Some of you who have been waiting the whole class to hear this -- just as readr can read in data from CSV (comma separated values) files, so it can write data back out to CSV and other text files so you can continued your analysis or visualization in other programs. This code will save the result above to a CSV file in your working directory:
```{r}
# write data to a csv file
write_csv(long_life, "long_life.csv", na="")
```

Although we have no null values here, including na="" is good practice, because it ensures that any empty cells in the data frame are saved as blanks, not NA.

# Group and summarize data
```[r}
# summarize the data by year, finding the maximum and minimum country-level life expectancies, and then calculate the range of values
longevity_summary <- nations1 %>%
  filter(!is.na(life_expect)) %>%
  group_by(year) %>%
  summarize(countries = n(),
            max_life_expect = max(life_expect),
            min_life_expect = min(life_expect)) %>%
  mutate(range_life_expect = max_life_expect - min_life_expect) %>%
  arrange(desc(year))
  #look at data
print(tbl_df(longevity_summary), n=25)

```
Notice that 
* 'mutate' is how we calculate a column based on other columns here
* summarize is how we operate on grouped values 
* arrange is how we sort and descending has to be specified with desc() -- by default it's ascending order (smallest to largest)

# Join data from two data frames

There are also a number of join functions in dplyr to combine data from two data frames. Here are the most useful:

* inner_join() returns values from both tables only where there is a match.
* left_join() returns all the values from the first-mentioned table, plus those from the second table that match.
* semi_join() filters the first-mentioned table to include only values that have matches in the second table.
* anti_join() filters the first-mentioned table to include only values that have no matches in the second table.

 [Here is a useful reference](http://stat545.com/bit001_dplyr-cheatsheet.html) for managing joins with dplyr.

 





R is a hot topic right now -- data journalists are increasingly learning it as a prime analysis tool, certainly for publishing analyses that can be easily replicated, and this is raising the bar for replicability everywhere.
When the Associated Press recently added new data journalism items to its stylebook, it [specified](www.niemanlab.org/2017/05/the-ap-stylebook-now-includes-new-guidelines-on-data-requesting-it-scraping-it-reporting-on-it-and-publishing-it/) that " “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”

“If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”
One journalism professor I know saw this and reacted this way: "I think this is a knife in the back of old school tinkering in Excel for a while until we get some numbers that look good. How do you replicate noodling around in a spreadsheet for a few hours? You can't. So we're going to have to get better about using methods that can be replicated. "
One of the nice things about R is that it shows every step you take from raw data to finished product -- much the way SAS or in some cases SPSS did, for those familiar with those statistics programs. And it's FREE while SAS and SPSS have become insupportably expensive for most of those outside academia. 


Go to 

Now, this is not written in stone. Many times you can replicate what was done in Excel with careful recordkeeping, and sometimes factcheckers will want to replicate your work even in getting the raw data together -- did you miss anything or fail to get part of the data? It happens! And even without R you would hope somebody could recreate your conclusions knowing nothing but your data source. 
But there's not doubt that there's great peace of mind in code that starts with raw data. I have an ex-colleague, now at the Washington Post, who says "Everytime I decide to just do something real quick in Excel, I always regret it." 
And I know what he means. Often a data request will seem to be a quick table, and you can bypass the steps of importing data, writing queries in SAS or SQL and just paste some numbers into a sheet and work with them a little and hand them over. But so many times you'll get follow-up requests -- "Geen, it would be nice to see this by year going back a ways, or could you just throw in a couple more stats..." 
And as a practical matter this could mean starting over, whereas if you've really datafied it by importing data, it's easy to expand or modify  what you've done, and there's less last-minute worry about mistakes.
I have a friend at Reuters who recommends writing down every sentence in a story based on data, then trying to recreate it again from scratch before publication without any reuse of code. 

So replication is not innoculation against mistakes, but it is a paper trail you can follow to isolate mistakes when they appear and fix thenm without starting over. And that's the real peace of mind. And there's nothing to say you can't use Excel, SQL, Perl or anything else you want to prepare your data for analysis in R. 

Also, with R you can benefit from others' analyses and follow along with the increasoing number of data journalism outlets that publish in R alongside their reports. 
For instance BuzzFeed and FiveThirtyEight keep a lot of their analyses online for anyone to replicate in R. 
* [FiveThirtyEight](https://github.com/fivethirtyeight/data)
* [BuzzFeed](https://github.com/BuzzFeedNews/everything)

# Getting Set up

There are reasons why [R is hard to learn](http://r4stats.com/articles/why-r-is-hard-to-learn/) -- it doesn't follow the mold of other data analysis or statistics programs and has its own syntax and keywords that often don't match what you'd expect, like "sort" -- 

