Intro to R
================

**By [Tim Henderson](https://twitter.com/TimHendersonSL) (tim dot hendo at gmail dot com)**
*Storytelling with Data Workshop at Boston University (June 12, 2017)* <br><br>

\*GitHub repository for Data+Code: <https://github.com/timhendo/R-intro>

Based on a tip sheet from Peter Aldhous, NICAR 2017, National Institute for Computer Assisted Reporting, with modifications

# Getting Set Up
Click on the link for the data and code in Github and download the zip file to your hard drive, then decompress to any directory you'll remember - c:/ is fine -- it would appear as c:/R-intro 

* Open RStudio. You should see three panes. 
* Go to File...New File and pick R Script. 

You already have an interactive environment where you can run commands from the script in the console below and see the results. To see this, type an equation at the prompt, then hit the RUN button at the top right. 

```{r}
1+1
```

You can also type 
x=1
and Run, then 
x
To see how it evaluates. 

The point of having a script separate from the console is that you can keep a record of what you did -- make the whole process easy to reproduce for somebody else. That's a big reason you'll want to learn and use R. 

So use File...Open to open the R-intro.R file in the directory we just downloaded. Now you can follow along with the data there.

And be sure to set the working directory so we can see our data. Go to Session...Set Working Directory...To Source File Location. (You could also Choose Directory if you hadn't already loaded a script from that directory)

# Save your data
The panel at top right has two tabs, the first showing the Environment, or all of the “objects” loaded into memory for this R session. We can save this as well, so we don’t have to load and process data again if we return to return to a project later.

Click on the save/disk icon in the Environment panel to save and call the file R-intro.RData.

# Comment your code

Anything that appears on a line after # will be treated as a comment, and will be ignored when the code is run. Get into the habit of commenting your code: Don’t trust yourself to remember what it does!


# Load R packages

Much of the power of R comes from the thousands of “packages” written by its community of open source contributors. These are optimized for specific statistical, graphical or data-processing tasks. To see what packages are available in the basic distribution of R, select the Packages tab in the panel at bottom right. To find packages for particular tasks, try searching Google using appropriate keywords and the phrase “R package.”

In this class, we will work with three incredibly useful packages developed by Hadley Wickham, chief scientist at RStudio:

* readr For reading and writing CSV and other text files.
* dplyr For processing and manipulating data.
* ggplot2 Charting library, which builds graphics in layers according to a standard grammar.

These and several other useful packages have been combined into a super-package called tidyverse.

To install a package, click on the Install icon in the Packages tab, type its name into the dialog box, and make sure that Install dependencies is checked, as some packages will only run correctly if other packages are also installed. Click Install and all of the required packages should install.

Each time you start R, it’s a good idea to click on Update in the Packages panel to update all your installed packages to the latest versions.

Installing a package makes it available to you, but to use it in any R session you need to load it. You can do this by checking its box in the Packages tab. However, we will enter the following code into our script, then highlight these lines of code and run them:

```{r}
# load required packages
library(readr)
library(dplyr)
library(ggplot2)
```
# Load data

You can load data into the current R session by selecting Import Dataset>From Text File... in the Environment tab.

However, we will use the read_csv function from the readr package. Copy the following code into your script and Run:

```{r}
# load health and wealth of nations data
nations1 <- read_csv("nations1.csv")
nations2 <- read_csv("nations2.csv")
```


Notice that the Environment now contains two objects, of the type tbl_df, a variety of the standard R object for holding tables of data, known as a data frame:

# Explore the data

The str function will tell you more about the columns in your data, including their data. 
```{r}
# view some of data
head(nations1)

```
The head() function lets you see the first six rows of data and also the data type, which in this scenario is guessed by the read() function. Not bad -- it correctly guess that some columns are character (text) and some are numbers -- though it's made the population column "double" for decimal when it's really an integer (no decimals)

Let's take a look at the population column specifically

```{r}
# print values for population in the nations1 data
nations1$population
```
The output will be the first 10,000 values for that column.

If you need to change the data type for any column, use the following functions:

* as.character converts to a text string.
* as.numeric converts to a number.
* as.factor converts to a categorical variable.
* as.integer converts to an integer
* as.Date converts to a date
* as.POSIXct converts to a full date and timestamp.

So this code will convert the population numbers to integers:

```{r}
# convert population to integers
nations1$population <- as.integer(nations1$population)
```
Now check again and note that population has changed to an integer type
```{r}
head(nations1)
```
Exercise: Try doing the same thing with the year column: look at it specifically, change it to an integer data type, then look at the nations1 data set again to make sure the change was made.

# Summarize the data

The summary function will run a quick statistical summary of a data frame, calculating mean, median and quartile values for continuous variables:
```{r}
# summary of nations1 data
summary(nations1)
```
Uh-oh! What's the income variable that's a character field? Sounds like it should be numeric! Let's check -- what should we do to look at that column specifically? 

# Manipulate and analyze data with dplyr

Now we will use dplyr to manipulate the data, using the following basic operations. If you’ve worked with SQL before, these operations will be very familiar and this is another reason Hadley Wickam is a genius and has turned R into something we all need:

* Sort: Largest to smallest, oldest to newest, alphabetical etc.

* Filter: Select a defined subset of the data.

* Summarize/Aggregate: Deriving one value from a series of other values to produce a summary statistic. Examples include: count, sum, mean, median, maximum, minimum etc. Often you’ll group data into categories first, and then aggregate by group.

* Join: Merging entries from two or more datasets based on common field(s), e.g. unique ID number, last name and first name.
 
For those with SQL backgrounds such as Microsoft Access or MySQL, this introduces some familiar concepts that will make R more hospitable. Here are some of the most useful functions in dplyr:

* select -- Choose which columns to include.
* filter -- Filter the data.
* arrange --Sort the data, by size for continuous variables, by date, or alphabetically.
* group_by-- Group the data by a categorical variable.
* summarize -- Summarize, or aggregate (for each group if following group_by). Often used in conjunction with functions including:
* mean(x) -- Calculate the mean, or average, for variable x.
* median(x) -- Calculate the median.
* max(x) -- Find the maximum value.
* min(x) -- Find the minimum value.
* sum(x) -- Add all the values together.
* n() -- Count the number of records. Here there isn’t a variable in the brackets of the function, because the number of records applies to all variables.
* n_distinct(x)--  Count the number of unique values in variable x.
* mutate -- Create new column(s) in the data, or change existing column(s).
* rename -- Rename column(s).

These functions can be chained together using the “pipe” operator %>%, which makes the output of one line of code the input for the next. This allows you to run through a series of operations in a logical order. I find it helpful to think of %>% as meaning “then.”

# Filter and sort data

Now we will filter and sort the data in specific ways. We create new objects -- the way nations1 is an object or data frame -- to hold our results. First, we'll filster 

First we'll Filter the data for 2014 only and for nations that have life expectancy data only.

```{r}
# filter data for 2014 only
longevity <- nations1 %>%
  filter(year == 2014 & !is.na(life_expect)) %>%
  select(country, life_expect, income, region)
  #check the result
  head(longevity)
  ```
  In this code, we create a new object called longevity from nations1 and then (%>%) filter it for just the data for 2015 and to include only non-null values. Then we select just four variables from the ten in the original data frame. There should be data returned for 195 countries.


Exercise: Wait, didn't we see 2015 as a year? Why are we using older data here? Try doing this for 2015 and see what happens.

Now, let find the top 10 high-income countries with the lowest life expectancy in 2014 

```{r}

# find the ten high-income countries with the lowest life expectancy
high_income_short_life <- longevity %>%
  filter(income == "High income") %>%
  arrange(life_expect) %>%
  head(10)
#Now look at what we have
head(high_income_short_life,10)
```

Next, Find the 20 countries with the longest life expectancy in 2014, plus the United States with its rank, if it lies outside the top 20
```{r)
# find the 20 countries with the longest life expectancies, 
# plus the United States with its rank, if it lies outside the top 20
long_life <- longevity %>%
  arrange(desc(life_expect)) %>%
  mutate(rank = c(1:195)) %>%
  filter(rank <= 20 | country == "United States")
  #take a look
  head(long_life,21)
```
Did that work? It did, but an RStudio element called tibbles prevents us from seeing all 21 rows, so we have to fall back on the print() function 
```[r}
#Tibble won't let us see that many rows so we fall back on print()
print(tbl_df(long_life), n=21)
```

Hopefully the logic and flexibility of dplyr code is now becoming clear. Here we start by sorting the longevity data frame in descending order of life expantancy, then we create a new variable in the data called rank, using the mutate function. By feeding this a list of numbers from 1 to 195, we rank the countries according to their life expectancy. Finally we filter the data for the top 20 countries, plus the United States.

Whereas in the initial filter of the data to create longevity data frame we used & to return data meeting both criteria, this time we used | to include data meeting either criteria. & and | are equivalent to AND and OR in SQL. When combining & and | in more complex filters, use parentheses to determine which parts of the evaluation should be carried out first.

Now let’s find out where Russia ranks, too
```{r}
# find the 20 countries with the longest life expectancies,
# plus the United States and Russia with their ranks
long_life <- longevity %>%
  arrange(desc(life_expect)) %>%
  mutate(rank = c(1:195)) %>%
  filter(rank <= 20 | grepl("United States|Russia", country))
  #Again use print to see all the rows
print(tbl_df(long_life), n=22)
  ```
This extremely geeky examples uses a UNIX style function for simple pattern matching on text, using the function grepl("pattern_a|pattern_b", x), which searches variable x for values containing any of a list of text values. This is useful for fuzzy text matching. Notice how searching for Russia returns Russian Federation, which is the country’s full name. 


# Write data to a CSV file (comma separated values)

Some of you who have been waiting the whole class to hear this -- just as readr can read in data from CSV (comma separated values) files, so it can write data back out to CSV and other text files so you can continued your analysis or visualization in other programs. This code will save the result above to a CSV file in your working directory:
```{r}
# write data to a csv file
write_csv(long_life, "long_life.csv", na="")
```

Although we have no null values here, including na="" is good practice, because it ensures that any empty cells in the data frame are saved as blanks, not NA.

# Group and summarize data
```[r}
# summarize the data by year, finding the maximum and minimum country-level life expectancies, and then calculate the range of values
longevity_summary <- nations1 %>%
  filter(!is.na(life_expect)) %>%
  group_by(year) %>%
  summarize(countries = n(),
            max_life_expect = max(life_expect),
            min_life_expect = min(life_expect)) %>%
  mutate(range_life_expect = max_life_expect - min_life_expect) %>%
  arrange(desc(year))
  #look at data
print(tbl_df(longevity_summary), n=25)

```
Notice that 
* 'mutate' is how we calculate a column based on other columns here
* summarize is how we operate on grouped values 
* arrange is how we sort and descending has to be specified with desc() -- by default it's ascending order (smallest to largest)

# Join data from two data frames

There are also a number of join functions in dplyr to combine data from two data frames. Here are the most useful:

* inner_join() returns values from both tables only where there is a match.
* left_join() returns all the values from the first-mentioned table, plus those from the second table that match.
* semi_join() filters the first-mentioned table to include only values that have matches in the second table.
* anti_join() filters the first-mentioned table to include only values that have no matches in the second table.

 [Here is a useful reference](http://stat545.com/bit001_dplyr-cheatsheet.html) for managing joins with dplyr.

 This code will join nations2 to nations1
```{r}
#join nations and nations2
nations <- inner_join(nations1, nations2)
```
Those used to a lot of specificity when joining will be amazed at the guesswork involved here, as they were were read_csv() to get data from a text file, but here it works -- it picks the matching elemenats and joins them
By default, dplyr looks for variables with matching names, here iso3c and year, and joins on those. But you can also specify exactly how a join should be made, like this:
```{r}
nations <- inner_join(nations1, nations2, by = c("iso3c" = "iso3c", "year" = "year"))
```
# Calculate total GDP by region and year

In the joined data frame, we can now calculate the total GDP for each country and each year, in trillions of dollars, and then add up the totals by region over the years:
```{r}
# total GDP, in trillions of dollars, by region, over time
gdp_regions <- nations %>%
  mutate(gdp = gdp_percap * population,
         gdp_tn = gdp/1000000000000) %>%
  group_by(region, year) %>%
  summarize(total_gdp_tn = sum(gdp_tn, na.rm = TRUE))
  ```
 Notice that variables created within a mutate function can be immediately used within the same function.

Here the group_by() function groups on two variables, region and year.

Notice that the sum function used to add up the GDP values across countries within each region and year includes the argument na.rm = TRUE, to remove the NA values before running the calculation.
Exercise: see what happens if you don't inlclude that
(Previously this wasn’t necessary because I had started by filtering out the NAs.)

Get into the habit of including na.rm = TRUE in your summary functions, to avoid problems cuased by null values!

# Now make some simple charts

Introducing ggplot2 and the grammar of graphics

The “gg” in ggplot2 stands for “grammar of graphics,” an approach to drawing charts devised by the statistician Leland Wilkinson. Rather than thinking in terms of finished charts like a scatter plot or a column chart, it starts by defining the coordinate system (usually the X and Y axes), maps data onto those coordinates, and then adds layers such as points, bars and so on. This is the logic behind ggplot2 code.

ggplot2 code basics:

* ggplot This is the master function that creates a ggplot2 chart.
* aes This function, named for “aesthetic mapping,” is used whenever data values are mapped onto a chart. So it is used when you define which variables are plotted onto the X and Y axes, and also if you want to change the size or color of parts of the chart according to values for a variable.
* geom All of the functions that add layers to a chart start with geom, followed by an underscore, for example geom_point() or geom_bar(). The code in the brackets for any geom layer styles the items in that layer, and can include aes mappings of values from data.
* theme This function modifies the appearance of elements of a plot. It is used, for example, to set size and font face for text, the position of a legend, and so on.
* scale Functions that begin with scale, followed by an underscore, are used to modify the way an aes mapping of data appears on a chart. They can change the axis range, for example, or specify a color palette to be used to encode values in the data.
* + is used each time you add a layer, a scale, a theme, or elements like axis labels and a title After a + you can continue on the same line of code or move the next line. I usually write a new line after each +, which makes the code easier to follow.

Draw a line chart showing total GDP, by region, over time.
```{r}
# Draw a line chart showing total GDP, by region, over time.
ggplot(gdp_regions, aes(x=year, y=total_gdp_tn, color=region)) +
  geom_line(size=1) +
  xlab("") +
  ylab("Total GDP ($ trillions)") +
  theme_minimal(base_size = 12)
```
This simple code draws a chart from the gdp_regions data, putting year on the X axis, and total_gdp_tn on the Y. It codes the regions by color, and then adds a line for each region to the chart using geom_line(). xlab() and ylab control the axis labels, which would otherwise by the variable names, and theme_minimal replaces the default gray grid of a ggplot2 chart with a white background. base_size sets the default font size.

Now Draw a bar chart showing the ten high-income countries with the lowest life expectancies in 2014.
```{r}
#Bar chart of 10 high income countries with lowest life expectancies
ggplot(high_income_short_life, aes(x=reorder(country,-life_expect), y=life_expect)) +
  geom_bar(stat="identity", fill = "red", alpha = 0.7) +
  xlab("") +
  ylab("Life expectancy at birth (2014)") + 
  ggtitle("Rich countries with low life expectancies") +
  theme_minimal(base_size = 12) +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank()) +
  coord_flip()

```
Exercise: try this on your own, making a new R script and pasting the commands into this as 
you go and running them. 

# Load California kindergarten immunization data
 
Now we’ll work with the California immunization data.
```{r}
# load data
immun <- read_csv("kindergarten.csv",  col_types = list(
  county = col_character(),
  district = col_character(),
  sch_code = col_character(),
  pub_priv = col_character(),
  school = col_character(),
  enrollment = col_integer(),
  complete  = col_integer(),
  start_year = col_integer()))

immun_2015 <- read_csv("kindergarten_2015.csv",  col_types = list(
  county = col_character(),
  district = col_character(),
  sch_code = col_character(),
  pub_priv = col_character(),
  school = col_character(),
  enrollment = col_integer(),
  complete  = col_integer(),
  start_year = col_integer()))
  ```
  
We need to append the data for 2015 to the older data. So this code specifies the data type for each variable, to be sure that there won’t be any mismatches in data type that would cause an error in the next step.
```{r}
# Append the 2015 data to the older data
# append the 2015 data to the older data
immun <- bind_rows(immun, immun_2015)
```


This code introducesbind_rows(), which appends one data frame to another, based on matching column names and data types. (If a column exists in one data frame but not in the other, NAs will be added where necessary.)

Calculate the percentage of children with incomplete immunizations, for the entire state, and by county

The data contains the number of children enrolled in each kindergarten across the state, and the number who has the complete recommended immunizations at the start of the year.

From this, we can calculate the percentage of children who did not have the complete achedule of immunizations. The following code runs these calculations for each year, first for the entire state, summing across all kindergartens grouped by year, and then for each of California’s 58 counties, by changing the group_by function.
```{r}
# percentage incomplete, entire state, by year
immun_year <- immun %>%
  group_by(start_year) %>%
  summarize(enrolled = sum(enrollment, na.rm=TRUE),
            completed = sum(complete, na.rm=TRUE)) %>%
  mutate(pc_incomplete = round(((enrolled-completed)/enrolled*100),2))

# percentage incomplete, by county, by year
immun_counties_year <- immun %>%
  group_by(county,start_year) %>%
  summarize(enrolled = sum(enrollment, na.rm = TRUE),
            completed = sum(complete, na.rm = TRUE)) %>%
  mutate(pc_incomplete = round(((enrolled-completed)/enrolled*100),2))
  ```
  
Notice how the round(x),n function is used to round values for x, here the percentage incomplete calculation, to n decimal places, here 2. Using negative numbers for n will round to tens, hundreds, and so on.

Now we can identify the five largest counties with the largest enrollment over the years, and use a join to filter the data by counties for just these five:
```{r}
# identify five counties with the largest enrollment over all years
top5 <- immun %>%
  group_by(county) %>%
  summarize(enrolled = sum(enrollment, na.rm = TRUE)) %>%
  arrange(desc(enrolled)) %>%
  head(5) %>%
  select(county)

# proportion incomplete, top 5 counties by enrollment, by year
immun_top5_year <- semi_join(immun_counties_year, top5)
```

Notice the use of semi_join() to filter the data for just the five counties with the largest kindergarten enrollment.


Make a series of charts to analyze this summarized data
Column chart by year, entire state
```{r}
# column chart by year, entire state
ggplot(immun_year, aes(x = start_year, y = pc_incomplete)) + 
  geom_bar(stat = "identity", fill = "red", alpha = 0.7) +
  xlab("") +
  ylab("Percent incomplete") +
  ggtitle("Immunization in California kindergartens, entire state") + 
  theme_minimal(base_size = 12) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```


# Dot-and-line chart by year, top 5 counties
```{r}
# dot and line chart by year, top5 counties
ggplot(immun_top5_year, aes(x = start_year, y = pc_incomplete, color = county)) + 
  geom_line(size = 1) +
  geom_point(size = 3) +
  xlab("") +
  ylab("Percent incomplete") +
  ggtitle("Immunization in California kindergartens\n(five largest counties)") +
  theme_minimal(base_size = 12)
```
Notice that this chart has two data layers, geom_point() and geom_line. Notice also how \n can be used in text, here in the chart title, to insert a line break.

# Heat map by year, all counties

```{r}
# heat map, all counties, by year
ggplot(immun_counties_year, aes(x = start_year, y = county)) +
  geom_tile(aes(fill = pc_incomplete), colour = "white") +
  scale_fill_gradient(low = "white",
                      high = "red", 
                      name="",
                      limits = c(0,35)) +
  xlab("") +
  ylab("County") +
  ggtitle("Immunization in California kindergartens, by county") +
  theme_minimal(base_size = 10) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position="bottom",
        legend.key.height = unit(0.4, "cm"))
```



R is a hot topic right now -- data journalists are increasingly learning it as a prime analysis tool, certainly for publishing analyses that can be easily replicated, and this is raising the bar for replicability everywhere.
When the Associated Press recently added new data journalism items to its stylebook, it [specified](www.niemanlab.org/2017/05/the-ap-stylebook-now-includes-new-guidelines-on-data-requesting-it-scraping-it-reporting-on-it-and-publishing-it/) that " “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”

“If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”
One journalism professor I know saw this and reacted this way: "I think this is a knife in the back of old school tinkering in Excel for a while until we get some numbers that look good. How do you replicate noodling around in a spreadsheet for a few hours? You can't. So we're going to have to get better about using methods that can be replicated. "
One of the nice things about R is that it shows every step you take from raw data to finished product -- much the way SAS or in some cases SPSS did, for those familiar with those statistics programs. And it's FREE while SAS and SPSS have become insupportably expensive for most of those outside academia. 


Now, this is not written in stone. Many times you can replicate what was done in Excel with careful recordkeeping, and sometimes factcheckers will want to replicate your work even in getting the raw data together -- did you miss anything or fail to get part of the data? It happens! And even without R you would hope somebody could recreate your conclusions knowing nothing but your data source. 
But there's not doubt that there's great peace of mind in code that starts with raw data. I have an ex-colleague, now at the Washington Post, who says "Everytime I decide to just do something real quick in Excel, I always regret it." 
And I know what he means. Often a data request will seem to be a quick table, and you can bypass the steps of importing data, writing queries in SAS or SQL and just paste some numbers into a sheet and work with them a little and hand them over. But so many times you'll get follow-up requests -- "Geen, it would be nice to see this by year going back a ways, or could you just throw in a couple more stats..." 
And as a practical matter this could mean starting over, whereas if you've really datafied it by importing data, it's easy to expand or modify  what you've done, and there's less last-minute worry about mistakes.
I have a friend at Reuters who recommends writing down every sentence in a story based on data, then trying to recreate it again from scratch before publication without any reuse of code. 

So replication is not innoculation against mistakes, but it is a paper trail you can follow to isolate mistakes when they appear and fix thenm without starting over. And that's the real peace of mind. And there's nothing to say you can't use Excel, SQL, Perl or anything else you want to prepare your data for analysis in R. 

Also, with R you can benefit from others' analyses and follow along with the increasoing number of data journalism outlets that publish in R alongside their reports. 
For instance BuzzFeed and FiveThirtyEight keep a lot of their analyses online for anyone to replicate in R. 
* [FiveThirtyEight](https://github.com/fivethirtyeight/data)
* [BuzzFeed](https://github.com/BuzzFeedNews/everything)

Learning curve? There are reasons why [R is hard to learn](http://r4stats.com/articles/why-r-is-hard-to-learn/) -- it doesn't follow the mold of other data analysis or statistics programs and has its own syntax and keywords that often don't match what you'd expect, like "sort" -- often helpful to look for R training aimed at people with your own background 

